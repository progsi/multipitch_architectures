{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute Features\n",
    "  \n",
    "This notebook shows how to compute an initial feature representation (Harmonic Constant-Q Transform or HCQT) from an audio file and how to convert multi-pitch annotations from a csv list of note events to an output representation for training multi-pitch estimators.\n",
    "\n",
    "&copy; Christof Weiss and Geoffroy Peeters, Télécom Paris 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "basepath = os.path.abspath(os.path.dirname(os.path.dirname('.')))\n",
    "sys.path.append(basepath)\n",
    "import numpy as np, os, scipy, scipy.spatial, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "#from numba import jit\n",
    "import librosa\n",
    "import libfmp.b, libfmp.c3, libfmp.c5\n",
    "import pandas as pd, pickle, re\n",
    "# from numba import jit\n",
    "from libdl.data_preprocessing import compute_hopsize_cqt, compute_hcqt, compute_efficient_hcqt, compute_annotation_array_nooverlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load audio\n",
    "\n",
    "Load an audio file, e.g. from the [MusicNet](https://doi.org/10.5281/zenodo.5139893) dataset. This serves to illustrate the extracted representations and the corresponding pitch annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 22050\n",
    "\n",
    "audio_folder = os.path.join(basepath, 'data', 'MusicNet', 'audio')\n",
    "fn_audio = '2382_Beethoven_OP130_StringQuartet.wav'\n",
    "\n",
    "path_audio = os.path.join(audio_folder, fn_audio)\n",
    "f_audio, fs_load = librosa.load(path_audio, sr=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute HCQT representation and plot its channels (corresponding to the harmonics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_per_semitone = 3\n",
    "num_octaves = 6\n",
    "n_bins = bins_per_semitone*12*num_octaves\n",
    "num_harmonics = 5\n",
    "num_subharmonics = 1\n",
    "\n",
    "f_hcqt, fs_hcqt, hopsize_cqt = compute_efficient_hcqt(f_audio, fs=22050, fmin=librosa.note_to_hz('C1'), fs_hcqt_target=50, \\\n",
    "                                                    bins_per_octave=bins_per_semitone*12, num_octaves=num_octaves, \\\n",
    "                                                    num_harmonics=num_harmonics, num_subharmonics=num_subharmonics)\n",
    "\n",
    "start_sec = 0\n",
    "show_sec = 3.5\n",
    "\n",
    "for curr_ax in range(0, 6):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 0.05]}, figsize=(10, 3.5))\n",
    "    im = libfmp.b.plot_matrix(np.log(1+1000*np.abs(f_hcqt[:, int(start_sec*fs_hcqt):int(show_sec*fs_hcqt), curr_ax])), Fs=fs_hcqt, ax=ax, cmap='gray_r', ylabel='MIDI pitch')\n",
    "    ax[0].set_yticks(np.arange(1, n_bins+13, 12*bins_per_semitone))\n",
    "    #ax[0].set_yticklabels([str(24+12*octave) for octave in range(0, num_octaves+2)])\n",
    "    ax[0].set_xticklabels(np.arange(start_sec-5, show_sec+5, 5))\n",
    "    if curr_ax==0:\n",
    "        ax[0].set_title('subharmonic 1')\n",
    "    elif curr_ax==1:\n",
    "        ax[0].set_title('harmonic 1 (fundamental)')\n",
    "    else:\n",
    "        ax[0].set_title('harmonic ' + str(curr_ax))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "### Optional: Save ###\n",
    "# path_output = ''\n",
    "# np.save(os.path.join(path_output, fn_audio[:-4]+'.npy'), f_hcqt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load annotations and convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_folder = os.path.join(basepath, 'data', 'MusicNet', 'csv')\n",
    "fn_annot = os.path.join(annot_folder, fn_audio[:-4]+'.csv')\n",
    "\n",
    "df = pd.read_csv(fn_annot, sep=',')\n",
    "note_events = df.to_numpy()[:,(0,1,3)]\n",
    "note_events[:,:2] /= 44100\n",
    "note_events = np.append(note_events, np.zeros((note_events.shape[0], 1)), axis=1)\n",
    "        \n",
    "f_annot_pitch = compute_annotation_array_nooverlap(note_events.copy(), f_hcqt, fs_hcqt, annot_type='pitch', shorten=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot annotation array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 0.05]}, figsize=(10, 3.5))\n",
    "\n",
    "cfig, cax, cim = libfmp.b.plot_matrix(f_annot_pitch[24:97, int(start_sec*fs_hcqt):int(show_sec*fs_hcqt)], ax=ax, Fs=fs_hcqt, cmap='gray_r', ylabel='MIDI pitch')\n",
    "plt.ylim([0, 73])\n",
    "ax[0].set_yticks(np.arange(0, 73, 12))\n",
    "ax[0].set_yticklabels([str(24+12*octave) for octave in range(0, num_octaves+1)])\n",
    "ax[0].set_title('Multi-pitch annotations (piano roll)')\n",
    "ax[1].set_ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "\n",
    "### Optional: Save ###\n",
    "# path_output_annot = ''\n",
    "# np.save(os.path.join(path_output_annot, song_fn_wav[:-4]+'.npy'), f_annot_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot annotations as overlay to CQT (fundamental), as in Fig. 2 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 0.05]}, figsize=(8, 5))\n",
    "f_log = np.log(1+70*np.abs(f_hcqt[:, :, 1]))\n",
    "f_log /= 0.96*np.max(f_log, axis=None)\n",
    "im = libfmp.b.plot_matrix(f_log, Fs=fs_hcqt, ax=ax, cmap='gray_r', ylabel='MIDI pitch', clim=[0, 1], colorbar=False)\n",
    "\n",
    "start_bin = 3*24 # MIDI pitch 24 + centered view\n",
    "bins_per_semitone = 3\n",
    "pitch_alpha = np.zeros((f_hcqt.shape[0], f_annot_pitch.shape[1], 4))\n",
    "for i in range(int((f_hcqt.shape[0]+start_bin)/bins_per_semitone)):\n",
    "    cqtLine = i*bins_per_semitone - start_bin\n",
    "    pitch_alpha[cqtLine,:,0:3] = [1, 0.1, 0.1]\n",
    "    pitch_alpha[cqtLine,:,3] = f_annot_pitch[i,:]*1\n",
    "\n",
    "T_coef = np.arange(f_hcqt.shape[1]) / fs_hcqt\n",
    "x_ext1 = (T_coef[1] - T_coef[0]) / 2\n",
    "x_ext2 = (T_coef[-1] - T_coef[-2]) / 2\n",
    "ax[0].imshow(pitch_alpha[::-1,:,:], extent=[T_coef[0] - x_ext1, T_coef[-1] + x_ext2, 0, f_hcqt.shape[0]], aspect='auto')\n",
    "        \n",
    "ax[0].set_yticks(np.arange(0, n_bins+1, 12*bins_per_semitone))\n",
    "ax[0].set_yticklabels([str(24+12*octave) for octave in range(0, num_octaves+1)])\n",
    "ax[0].set_xlim([0, 3.5])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "### Optional: Save ###\n",
    "# path_output_image = ''\n",
    "# plt.savefig(path_output_image, dpi=600)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
