{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with Pre-trained Model\n",
    "  \n",
    "This notebook shows how to load a pre-trained model and to use this for predicting multi-pitch estimates of an unknown audio file.\n",
    "\n",
    "&copy; Christof Weiss and Geoffroy Peeters, Télécom Paris 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "basepath = os.path.abspath(os.path.dirname(os.path.dirname('.')))\n",
    "sys.path.append(basepath)\n",
    "import numpy as np, os, scipy, scipy.spatial, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "# from numba import jit\n",
    "import librosa\n",
    "import libfmp.b, libfmp.c3, libfmp.c5\n",
    "import pandas as pd, pickle, re\n",
    "# from numba import jit\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import libdl.data_preprocessing\n",
    "from libdl.data_loaders import dataset_context\n",
    "from libdl.nn_models import basic_cnn_segm_sigmoid, deep_cnn_segm_sigmoid, simple_u_net_largekernels\n",
    "from libdl.nn_models import simple_u_net_doubleselfattn, simple_u_net_doubleselfattn_twolayers\n",
    "from libdl.nn_models import u_net_blstm_varlayers, simple_u_net_polyphony_classif_softmax\n",
    "from libdl.data_preprocessing import compute_hopsize_cqt, compute_hcqt, compute_efficient_hcqt, compute_annotation_array_nooverlap\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Specify and load model (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_models = os.path.join(basepath, 'models_pretrained')\n",
    "num_octaves_inp = 6\n",
    "num_output_bins, min_pitch = 72, 24\n",
    "\n",
    "\n",
    "# Polyphony U-Net trained in recommended MusicNet split (test set MuN-10full):\n",
    "model_params = {'n_chan_input': 6,\n",
    "                'n_chan_layers': [128,180,150,100],\n",
    "                'n_ch_out': 2,\n",
    "                'n_bins_in': num_octaves_inp*12*3,\n",
    "                'n_bins_out': num_output_bins,\n",
    "                'a_lrelu': 0.3,\n",
    "                'p_dropout': 0.2,\n",
    "                'scalefac': 2,\n",
    "                'num_polyphony_steps': 24\n",
    "                }\n",
    "mp = model_params\n",
    "\n",
    "fn_model = 'RETRAIN4_exp195f_musicnet_aligned_unet_extremelylarge_polyphony_softmax_rerun1.pt'\n",
    "model = simple_u_net_polyphony_classif_softmax(n_chan_input=mp['n_chan_input'], n_chan_layers=mp['n_chan_layers'], \\\n",
    "    n_bins_in=mp['n_bins_in'], n_bins_out=mp['n_bins_out'], a_lrelu=mp['a_lrelu'], p_dropout=mp['p_dropout'], \\\n",
    "    scalefac=mp['scalefac'], num_polyphony_steps=mp['num_polyphony_steps'])\n",
    "\n",
    "\n",
    "path_trained_model = os.path.join(dir_models, fn_model)\n",
    "\n",
    "model.load_state_dict(torch.load(path_trained_model, map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()\n",
    "summary(model, input_size=(1, 6, 174, 216))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Test Audio and compute HCQT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 22050\n",
    "\n",
    "audio_folder = os.path.join(basepath, 'data', 'MusicNet', 'audio')\n",
    "fn_audio = '2382_Beethoven_OP130_StringQuartet.wav'\n",
    "\n",
    "path_audio = os.path.join(audio_folder, fn_audio)\n",
    "f_audio, fs_load = librosa.load(path_audio, sr=fs)\n",
    "\n",
    "bins_per_semitone = 3\n",
    "num_octaves = 6\n",
    "n_bins = bins_per_semitone*12*num_octaves\n",
    "num_harmonics = 5\n",
    "num_subharmonics = 1\n",
    "center_bins=True\n",
    "\n",
    "f_hcqt, fs_hcqt, hopsize_cqt = compute_efficient_hcqt(f_audio, fs=22050, fmin=librosa.note_to_hz('C1'), fs_hcqt_target=50, \\\n",
    "                                                    bins_per_octave=bins_per_semitone*12, num_octaves=num_octaves, \\\n",
    "                                                    num_harmonics=num_harmonics, num_subharmonics=num_subharmonics, center_bins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predict Pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set test parameters\n",
    "test_params = {'batch_size': 1,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 1\n",
    "              }\n",
    "device = 'cpu'\n",
    "\n",
    "test_dataset_params = {'context': 75,\n",
    "                       'stride': 1,\n",
    "                       'compression': 10\n",
    "                      }\n",
    "half_context = test_dataset_params['context']//2\n",
    "\n",
    "inputs = np.transpose(f_hcqt, (2, 1, 0))\n",
    "targets = np.zeros(inputs.shape[1:]) # need dummy targets to use dataset object\n",
    "\n",
    "inputs_context = torch.from_numpy(np.pad(inputs, ((0, 0), (half_context, half_context+1), (0, 0))))\n",
    "targets_context = torch.from_numpy(np.pad(targets, ((half_context, half_context+1), (0, 0))))\n",
    "\n",
    "test_set = dataset_context(inputs_context, targets_context, test_dataset_params)\n",
    "test_generator = torch.utils.data.DataLoader(test_set, **test_params)\n",
    "\n",
    "pred_tot = np.zeros((0, num_output_bins))\n",
    "\n",
    "max_frames = 160\n",
    "k=0\n",
    "for test_batch, test_labels in test_generator:\n",
    "    k+=1\n",
    "    if k>max_frames:\n",
    "        break\n",
    "    # Model computations\n",
    "    y_pred, n_pred = model(test_batch)\n",
    "    pred_log = torch.squeeze(torch.squeeze(y_pred.to('cpu'),2),1).detach().numpy()\n",
    "    # pred_log = torch.squeeze(y_pred.to('cpu')).detach().numpy()\n",
    "    pred_tot = np.append(pred_tot, pred_log, axis=0)\n",
    "    \n",
    "predictions = pred_tot\n",
    "\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot Predictions and Annotations (Ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sec = 0\n",
    "show_sec = 3.5\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 0.05]}, figsize=(10, 3.5))\n",
    "im = libfmp.b.plot_matrix(predictions.T[:, int(start_sec*fs_hcqt):int(show_sec*fs_hcqt)], Fs=fs_hcqt, ax=ax, cmap='gray_r', ylabel='MIDI pitch')\n",
    "ax[0].set_yticks(np.arange(0, 73, 12))\n",
    "ax[0].set_yticklabels([str(24+12*octave) for octave in range(0, num_octaves+1)])\n",
    "ax[0].set_title('Multi-pitch predictions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compare with annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_folder = os.path.join(basepath, 'data', 'MusicNet', 'csv')\n",
    "fn_annot = os.path.join(annot_folder, fn_audio[:-4]+'.csv')\n",
    "\n",
    "df = pd.read_csv(fn_annot, sep=',')\n",
    "note_events = df.to_numpy()[:,(0,1,3)]\n",
    "note_events[:,:2] /= 44100\n",
    "note_events = np.append(note_events, np.zeros((note_events.shape[0], 1)), axis=1)\n",
    "        \n",
    "f_annot_pitch = compute_annotation_array_nooverlap(note_events.copy(), f_hcqt, fs_hcqt, annot_type='pitch', shorten=1.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 0.05]}, figsize=(10, 3.5))\n",
    "\n",
    "cfig, cax, cim = libfmp.b.plot_matrix(f_annot_pitch[24:97, int(start_sec*fs_hcqt):int(show_sec*fs_hcqt)], ax=ax, Fs=fs_hcqt, cmap='gray_r', ylabel='MIDI pitch')\n",
    "plt.ylim([0, 73])\n",
    "ax[0].set_yticks(np.arange(0, 73, 12))\n",
    "ax[0].set_yticklabels([str(24+12*octave) for octave in range(0, num_octaves+1)])\n",
    "ax[0].set_title('Multi-pitch annotations (piano roll)')\n",
    "ax[1].set_ylim([0, 1])\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
